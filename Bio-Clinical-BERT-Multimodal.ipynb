{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da7ce2-304f-4656-8dab-1b43a80c570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchmetrics import AUROC\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\"\"\"\n",
    "Script for Data Preprocessing and Tokenizer Initialization\n",
    "\n",
    "This script reads an Excel file, splits the dataset into training, validation, and test sets, and initializes a ClinicalBERT tokenizer.\n",
    "\n",
    "Data Structure:\n",
    "    - The target variable (label) for prediction is stored in a column named 'output' with 3 classes.\n",
    "    - The first feature column is clean-up operative notes.\n",
    "\n",
    "Functionalities:\n",
    "    1. Reads the Excel file into a Pandas DataFrame.\n",
    "    2. Splits the DataFrame into feature and label sets.\n",
    "    3. Further splits these into training, validation, and test sets.\n",
    "    4. Outputs the shapes of these datasets for verification.\n",
    "    5. Separates dynamic features from static features.\n",
    "    6. Initializes a ClinicalBERT tokenizer for further processing.\n",
    "    7. Tokenize the notes and create training, validation, and testing dataloaders.\n",
    "    8. Input dimensions are [512, 512, 75, 1].\n",
    "    9. First two dimensions represent the encoded input IDs and attention masks from Bio-Clinical BERT.\n",
    "    10. The third dimension represents the structured EHR data.\n",
    "    11. The fourth dimension represents the outcome labels.\n",
    "\"\"\"\n",
    "\n",
    "# Read the Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel('glaucoma_surgery_dataset.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Isolate the target variable (label) which we want to predict\n",
    "labels = df['output']\n",
    "# Remove the target variable from the feature set; axis=1 means we drop a column not a row\n",
    "features = df.drop('output', axis=1)\n",
    "\n",
    "# Split the data into a training set and a temporary validation/test set\n",
    "# We're using 30% of the data for the temporary validation/test set, stratified by the label\n",
    "train_features, val_test_features, train_labels, val_test_labels = train_test_split(features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the temporary validation/test set into validation and test sets\n",
    "# 2/3 of the data goes to the test set, stratified by the label\n",
    "val_features, test_features, val_labels, test_labels = train_test_split(val_test_features, val_test_labels, test_size=(2/3), random_state=42, stratify=val_test_labels)\n",
    "\n",
    "# Display shapes to ensure everything is as expected\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Val Features Shape:', val_features.shape)\n",
    "print('Val Labels Shape:', val_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "# Select static features by dropping the first column (operative note) from the train, validation and test feature sets\n",
    "train_static = train_features.iloc[:, 1:]\n",
    "val_static = val_features.iloc[:, 1:]\n",
    "test_static = test_features.iloc[:, 1:]\n",
    "\n",
    "# Initialize the ClinicalBERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "#Tokenize the operative notes using BertTokenizer from the Transformers library.\n",
    "\n",
    "#Generate the dataset and dataloader using TensorDataset and DataLoader from PyTorch.\n",
    "\n",
    "'''\n",
    "Sets random seeds to ensure reproducibility across different runs.\n",
    "'''\n",
    "# Set a random seed for reproducibility across runs\n",
    "random_seed = 101  # Or any other favorite number you prefer\n",
    "# Set the random seed for PyTorch (both CPU and CUDA)\n",
    "torch.manual_seed(random_seed)\n",
    "# Set the random seed for CUDA (GPU)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# Make CuDNN deterministic to ensure reproducibility\n",
    "# (may slow down the computations)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Enable CuDNN benchmarking for potentially better performance\n",
    "# (this should be enabled only if the input sizes do not vary)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(random_seed)\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(random_seed)\n",
    "\n",
    "\"\"\"\n",
    "Start to train the PyTorch model using a specified set of parameters, optimizer, and loss function.\n",
    "\"\"\"\n",
    "   \n",
    "# Check if CUDA is available and set the device to GPU if possible, else use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200  # Number of training epochs\n",
    "LEARNING_RATE = 4e-5  # Learning rate for the optimizer\n",
    "WEIGHT_DECAY = 1e-5  # Weight decay for L2 regularization\n",
    "class_weights = torch.tensor([0.2584, 0.8678, 0.8737]).to(device)  # Class weights for handling class imbalance\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)  # Cross-entropy loss function with class weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)  # Adam optimizer with learning rate and weight decay\n",
    "\n",
    "# Convert validation labels to a list\n",
    "# This is done to feed the labels into the evaluation function\n",
    "y_val = val_labels.tolist()\n",
    "\n",
    "# Train the model\n",
    "# Calls the 'train' function and stores returned training and validation losses\n",
    "train_losses, val_losses = train(model, optimizer, loss_fn, train_dataloader, val_dataloader, y_val, epochs, batchSize=16)\n",
    "\n",
    "# Plotting the losses\n",
    "# Plots both training and validation losses over epochs\n",
    "plt.plot(train_losses, label='train loss')  # Plot training losses\n",
    "plt.plot(val_losses, label='test loss')  # Plot validation/test losses\n",
    "plt.legend()  # Add legend to the plot\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Saving the trained model weights to disk\n",
    "PATH = 'Bio-Clinical-BERT-Mulimodal.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A PyTorch model class that combines a pre-trained Bio_ClinicalBERT model with static data and additional\n",
    "fully connected layers for classification tasks.\n",
    "\n",
    "Parameters:\n",
    "- bert_model_name (str): The name of the pre-trained BERT model to use. Default is 'emilyalsentzer/Bio_ClinicalBERT'.\n",
    "- classifier_dropout (float): Dropout rate for the fully connected layers. Default is 0.5.\n",
    "- n_node_layer1 (int): Number of nodes in the first fully connected layer. Default is 256.\n",
    "- n_node_layer2 (int): Number of nodes in the second fully connected layer. Default is 48.\n",
    "- static_size (int): The dimension of the static input features. Default is 75.\n",
    "\n",
    "Methods:\n",
    "- forward(input_ids, attention_mask, x_static): Forward pass for the model.\n",
    "\"\"\"\n",
    "\n",
    "class Multi_BERT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name='emilyalsentzer/Bio_ClinicalBERT',  # Pre-defined model name\n",
    "        classifier_dropout=0.5,\n",
    "        n_node_layer1=256,\n",
    "        n_node_layer2=48,\n",
    "        static_size=75):\n",
    "\n",
    "        super(Multi_BERT, self).__init__()\n",
    "\n",
    "        # Load the Bio_ClinicalBERT model\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        # Extract the hidden size from BERT's configurations (usually 768 for base models)\n",
    "        d_model = self.bert.config.hidden_size\n",
    "\n",
    "        # Linear layer to reduce the dimensionality of BERT's output\n",
    "        self.bert_output_reducer = nn.Linear(d_model, 50)\n",
    "\n",
    "        # Static data dimensions\n",
    "        self.static_size = static_size\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=classifier_dropout)\n",
    "\n",
    "        # Batch normalization layers to stabilize learning\n",
    "        self.batchnorm1 = nn.BatchNorm1d(n_node_layer1, momentum=0.1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(n_node_layer2, momentum=0.1)\n",
    "        \n",
    "        # Define the fully connected layers\n",
    "        self.linear1 = nn.Linear(50 + static_size, n_node_layer1)  # Combine BERT and static data\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.linear2 = nn.Linear(n_node_layer1, n_node_layer2)\n",
    "        self.classifier = nn.Linear(n_node_layer2, 3)  # Output layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, x_static):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass input through BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the [CLS] token's representation (used for classification tasks)\n",
    "        bert_output = outputs[0][:, 0, :]\n",
    "\n",
    "        # Reduce the dimensionality of the BERT output\n",
    "        bert_output = self.bert_output_reducer(bert_output)\n",
    "        \n",
    "        # Concatenate BERT output and static data for the fully connected layers\n",
    "        inputs = torch.cat([bert_output, x_static], dim=1)\n",
    "        \n",
    "        # Pass through the first fully connected layer and apply ReLU and BatchNorm\n",
    "        out = self.relu(self.linear1(inputs))\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Pass through the second fully connected layer and apply ReLU and BatchNorm\n",
    "        out = self.relu(self.linear2(out))\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Pass through the output layer\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\"\"\"\n",
    "Initialize and configure the Multi_BERT model, and move it to the specified device (CPU or GPU).\n",
    "Model (Multi_BERT): The initialized and configured Multi_BERT model moved to the specified device.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the Multi_BERT model with specified hyperparameters\n",
    "model = Multi_BERT(\n",
    "    bert_model_name='emilyalsentzer/Bio_ClinicalBERT',  # Pre-trained model to use\n",
    "    classifier_dropout=0.5,  # Dropout rate\n",
    "    n_node_layer1=256,  # Number of nodes in the first fully connected layer\n",
    "    n_node_layer2=48,   # Number of nodes in the second fully connected layer\n",
    "    static_size=75,     # Size of the static features\n",
    ")\n",
    "\n",
    "# Make all BERT model parameters trainable\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move the model to the appropriate processing device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "def get_accuracy(out, actual_labels, batchSize):\n",
    "    '''\n",
    "    Computes the accuracy of a model's predictions for a given batch.\n",
    "    \n",
    "    Parameters:\n",
    "    - out (Tensor): The log probabilities or logits returned by the model.\n",
    "    - actual_labels (Tensor): The actual labels for the batch.\n",
    "    - batchSize (int): The size of the batch.\n",
    "    \n",
    "    Returns:\n",
    "    float: The accuracy for the batch.\n",
    "    '''\n",
    "    # Get the predicted labels from the maximum value of log probabilities\n",
    "    predictions = out.max(dim=1)[1]\n",
    "    # Count the number of correct predictions\n",
    "    correct = (predictions == actual_labels).sum().item()\n",
    "    # Compute the accuracy for the batch\n",
    "    accuracy = correct / batchSize\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dataloader, val_dataloader, y_val, epochs=20, batchSize=16):\n",
    "    '''\n",
    "    Train a PyTorch model using the given parameters and dataloaders.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (nn.Module): The PyTorch model to train.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer for training.\n",
    "    - loss_fn (callable): The loss function.\n",
    "    - train_dataloader (DataLoader): The DataLoader for the training data.\n",
    "    - val_dataloader (DataLoader): The DataLoader for the validation data.\n",
    "    - y_val (array-like): The true labels for the validation data.\n",
    "    - epochs (int, optional): The number of training epochs. Default is 20.\n",
    "    - batchSize (int, optional): The size of each batch. Default is 16.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The training and validation losses for each epoch from validation.\n",
    "    '''\n",
    "    \n",
    "    # Initialize device to GPU if available, else CPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize metrics to track best validation loss, accuracy, and AUC\n",
    "    best_val_loss = 2\n",
    "    best_accuracy = 0\n",
    "    best_AUC = 0\n",
    "    best_p = []  # For storing best probability scores\n",
    "    \n",
    "    # Initialize arrays to store training and validation losses for each epoch\n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # Print the header for the training log\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^9} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val AUC':^9} | {'Val F1':^9} |{'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Loop through each epoch\n",
    "    for epoch_i in tqdm(range(epochs)):\n",
    "        # Record time at start of epoch\n",
    "        t0_epoch = time.time()\n",
    "        \n",
    "        # Initialize metrics for the current epoch\n",
    "        total_loss = 0\n",
    "        epoc_acc = 0\n",
    "        \n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop through each batch of data in the training dataloader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load the current batch\n",
    "            b_input_ids, b_attention_mask, b_input_tbl, b_labels = batch\n",
    "            b_input_ids, b_attention_mask, b_input_tbl, b_labels = b_input_ids.to(device), b_attention_mask.to(device), b_input_tbl.to(device), b_labels.long().to(device)\n",
    "            \n",
    "            # Clear the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(b_input_ids, b_attention_mask, b_input_tbl)\n",
    "            logits = logits.float().to(device)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            epoc_acc += get_accuracy(logits, b_labels, batchSize)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Compute average loss and accuracy over the epoch\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_train_acc = epoc_acc / len(train_dataloader)\n",
    "        train_losses[epoch_i] = avg_train_loss\n",
    "\n",
    "        # Validate the model if a validation dataloader is provided\n",
    "        if val_dataloader is not None:\n",
    "            val_loss, val_accuracy, val_AUC, val_f1, p = evaluate(model, val_dataloader, y_val, batchSize=16)\n",
    "            \n",
    "            val_losses[epoch_i] = val_loss\n",
    "\n",
    "            # Update best metrics if current epoch's metrics are better\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "            if val_AUC > best_AUC:\n",
    "                best_AUC = val_AUC\n",
    "                best_p = p\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_wts_BERTv1 = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            # Compute elapsed time for the epoch\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            # Log training and validation metrics\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {avg_train_acc:^9.2f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_AUC:^9.4f} | {val_f1:^9.4f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "    # Print final best metrics\n",
    "    print(f\"\\nTraining complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    print(f\"Training complete! Best AUC: {best_AUC:.4f}.\")\n",
    "    \n",
    "    # Return training and validation losses, and the best probability scores\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, val_dataloader, y_val, batchSize=16):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model to be evaluated.\n",
    "    - val_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "    - y_val (array): Array of validation labels.\n",
    "    - batchSize (int, optional): Batch size. Default is 16.\n",
    "\n",
    "    Returns:\n",
    "    - val_loss (float): Average validation loss.\n",
    "    - val_accuracy (float): Average validation accuracy.\n",
    "    - val_AUC (float): Area under the ROC curve for the validation set.\n",
    "    - val_f1 (float): F1 score for the validation set.\n",
    "    - p (array): Probabilities of each class for each sample in the validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store various metrics\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    outputs_list = []\n",
    "    y_pred_list = []\n",
    "    probs_list = []\n",
    "\n",
    "    # Initialize tensor placeholders for predictions and true labels\n",
    "    preds_list = torch.tensor([], dtype=torch.long, device=device)\n",
    "    labels_list = torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "    # Loop through each batch of data in the validation dataloader\n",
    "    for batch in val_dataloader:\n",
    "        # Load the current batch\n",
    "        b_input_ids, b_attention_mask, b_input_tbl, b_labels = batch\n",
    "        b_input_ids, b_attention_mask, b_input_tbl, b_labels = b_input_ids.to(device), b_attention_mask.to(device), b_input_tbl.to(device), b_labels.long().to(device)\n",
    "\n",
    "        # Forward pass with no gradient calculation\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attention_mask, b_input_tbl)\n",
    "            logits = logits.float().to(device)\n",
    "        \n",
    "        # Compute softmax probabilities\n",
    "        y_val_probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        \n",
    "        # Append logits and probabilities to respective lists\n",
    "        outputs_list.append(logits)\n",
    "        probs_list.append(y_val_probs)\n",
    "        \n",
    "        # Extract predicted labels (class with max logit)\n",
    "        predictions = logits.max(dim=1)[1]\n",
    "        \n",
    "        # Concatenate predictions and true labels for this batch to existing list\n",
    "        preds_list = torch.cat([preds_list, predictions])\n",
    "        labels_list = torch.cat([labels_list, b_labels])\n",
    "\n",
    "        # Compute loss for this batch and store\n",
    "        loss = loss_fn(logits, b_labels).to(device)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Compute accuracy for this batch and store\n",
    "        val_accuracy.append(get_accuracy(logits, b_labels, batchSize))\n",
    "\n",
    "    # Compute mean loss and accuracy for entire validation set\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    # Convert lists to NumPy arrays for evaluation\n",
    "    p = torch.cat(probs_list).detach().cpu().numpy()\n",
    "    preds_list = preds_list.cpu().numpy()\n",
    "    labels_list = labels_list.cpu().numpy()\n",
    "\n",
    "    # Compute F1 score and AUC\n",
    "    val_f1 = f1_score(labels_list, preds_list, average='weighted')\n",
    "    val_AUC = roc_auc_score(y_val, p, multi_class='ovr')\n",
    "\n",
    "    # Return all evaluation metrics\n",
    "    return val_loss, val_accuracy, val_AUC, val_f1, p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcfc4fe-483f-47c3-b247-9c1737bc8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This following code performs the following operations:\n",
    "\n",
    "1. Load the pre-trained model from the specified file.\n",
    "2. Evaluate the model on a test set to generate predictions.\n",
    "3. Compute and plot Receiver Operating Characteristic (ROC) curves for each class and their macro-average.\n",
    "4. Compute and plot Precision-Recall (P-R) curves for each class and their macro-average.\n",
    "5. Print out the classification report and the confusion matrix for model evaluation.\n",
    "\n",
    "Outputs:\n",
    "- Plots of ROC and P-R curves.\n",
    "- Printed classification report and confusion matrix.\n",
    "\"\"\"\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('Bio-Clinical-BERT-Mulimodal.pth'))\n",
    "\n",
    "# Get predictions for test set\n",
    "model.eval()\n",
    "test_probabilities = []\n",
    "test_true_labels = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_input_tbl = batch[2].to(device)\n",
    "    b_labels = batch[3].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, b_input_mask, b_input_tbl)\n",
    "        logits = logits.float().to(device)\n",
    "    \n",
    "    #logits = outputs[0]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    test_probabilities.extend(probs.detach().cpu().numpy())\n",
    "    test_true_labels.extend(b_labels.detach().cpu().numpy())\n",
    "\n",
    "test_probabilities = np.array(test_probabilities)\n",
    "test_true_labels = np.array(test_true_labels)\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "all_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_true_labels == i, test_probabilities[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class {0} (area = {1:0.4f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(3):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= 3\n",
    "mean_auc = auc(all_fpr, mean_tpr)\n",
    "plt.plot(all_fpr, mean_tpr, color='b', linestyle='--', lw=2, label='Macro-average ROC (area = {0:0.4f})'.format(mean_auc))\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.show()\n",
    "\n",
    "# Compute macro-average P-R curve and P-R area\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(3):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(test_true_labels == i, test_probabilities[:, i])\n",
    "    average_precision[i] = average_precision_score(test_true_labels == i, test_probabilities[:, i])\n",
    "    plt.step(recall[i], precision[i], lw=2, where='post', label='P-R curve of class {0} (area = {1:0.4f})'.format(i, average_precision[i]))\n",
    "\n",
    "# Macro-average P-R curve and P-R area\n",
    "mean_precision = sum(average_precision.values()) / 3\n",
    "plt.plot([0, 1], [mean_precision, mean_precision], linestyle='--', lw=2, color='b', label='Macro-average P-R (area = {0:0.4f})'.format(mean_precision))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Precision-Recall (P-R) Curves')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "predicted_classes = np.argmax(test_probabilities, axis=1)\n",
    "print(classification_report(test_true_labels, predicted_classes, target_names=['Success', 'Low IOP', 'High IOP']))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(test_true_labels, predicted_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
